{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7e90fa",
   "metadata": {
    "id": "ccac684e"
   },
   "source": [
    "### Generating Text with the Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bfd12e7",
   "metadata": {
    "id": "45bd5d3b"
   },
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('problem1_data/RNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "RNN_model = model_from_json(loaded_model_json)\n",
    "RNN_model.load_weights(\"problem1_data/RNN_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fcb62d5",
   "metadata": {
    "id": "648273ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Vanilla_RNN_1 (SimpleRNN)   (None, 100, 128)          22144     \n",
      "                                                                 \n",
      " Vanilla_RNN_2 (SimpleRNN)   (None, 64)                12352     \n",
      "                                                                 \n",
      " Dense_layer (Dense)         (None, 44)                2860      \n",
      "                                                                 \n",
      " Softmax_layer (Activation)  (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,356\n",
      "Trainable params: 37,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load in the weights and show summary\n",
    "weights_RNN = RNN_model.get_weights()\n",
    "RNN_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa1ed9",
   "metadata": {
    "id": "a4bcccd8"
   },
   "source": [
    "Finish the following function that uses a vanilla RNN architecture to generate text, given the weights of the RNN model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the RNN architecture. Do not worry about the weight extraction part, this is already provided for you. The weight matrix $W_{xh1}$, for example, denotes the weight matrix to go from the input x to the first hidden state layer h1. The hidden states $h_1$ and $h_2$ are initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2be298b1",
   "metadata": {
    "id": "736ff633"
   },
   "outputs": [],
   "source": [
    "def sample_text_RNN(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained RNN to generate text, starting from a prompt, \n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained RNN model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Length of output sentence (including prompt)\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by RNN\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    output_sentence = prompt\n",
    "    \n",
    "    for i in range(N - len(prompt)):\n",
    "        x = np.zeros((W_xh1.shape[1], 1))\n",
    "        current_char = output_sentence[-1] if output_sentence else ' '\n",
    "        x[char_to_indices[current_char]] = 1\n",
    "        \n",
    "        # Recurrent layers\n",
    "        h1 = np.tanh(np.dot(W_xh1, x) + np.dot(W_h1h1, h1) + b_h1)\n",
    "        h2 = np.tanh(np.dot(W_h1h2, h1) + np.dot(W_h2h2, h2) + b_h2)\n",
    "            \n",
    "        # Output\n",
    "        y = np.dot(W_h2y, h2) + b_y\n",
    "        exp_y = np.exp(y - np.max(y))\n",
    "        p = exp_y / np.sum(exp_y)\n",
    "        \n",
    "        # Sample the next character based on the probability distribution\n",
    "        next_char_index = np.random.choice(len(p), p=p.flatten())\n",
    "        next_char = indices_to_char[next_char_index]\n",
    "        \n",
    "        # Append the next character to the output sentence\n",
    "        output_sentence += next_char\n",
    "        \n",
    "        # Update the current character\n",
    "        current_char = next_char\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bf9d6",
   "metadata": {
    "id": "cfc7420f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ff731ee",
   "metadata": {
    "id": "12458ad0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. oliver looked at herself in the glass. she gave a brief, sideways lookith could be mrs. celia. \"you anywaress. hould said mry. she'd beem see to de a get really as i mest they have know the was she was, combarilden-she macanceding,\" said mrs. burbef-celinit. anyon't it?\" eaplise. the peoplivellytsing. she was\"s who llue,\" said she teven adyt at is wis whem knew, \"andicady or something gen,\" said you hore he heard inttibliwh i'm some to have at-wo mis. ouncearsit. and silved youl them would know it overy,\" said mrs. oliver. \"and it con't know. i didn't lealing really. mrs. oliver hore knows, radddause interventice,\" said garreling eats exednes mostaly ot soon of owl nwelly ments, ftrause and asked from that there't was pes'abseaply extenssed wight find, you know the om nine, you might quike of them, or somrofios, sure office by my very offeeved her yous she nut mand aso and the deass. i think an adeain--\"o\" \"yes,\" said mrs. oliver, wasking at. all nid cone to of this and so most se\n"
     ]
    }
   ],
   "source": [
    "print(sample_text_RNN(weights_RNN, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look', \n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ffaf0",
   "metadata": {
    "id": "c950eae8"
   },
   "source": [
    "### Generating Text with the GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27b1fc",
   "metadata": {
    "id": "1dd93ee7"
   },
   "source": [
    "The code below loads in a pretrained GRU model. The model is set up exactly like in the lecture slides (with sigmoid activation layers for the gates and tanh activation layers in the recurrent layer). The model is trained for only 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36b90214",
   "metadata": {
    "id": "82291909"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 13:07:47.973854: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-11-25 13:07:47.974542: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-11-25 13:07:47.975093: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('problem1_data/GRU_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "GRU_model = model_from_json(loaded_model_json)\n",
    "GRU_model.load_weights(\"problem1_data/GRU_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "735f4c88",
   "metadata": {
    "id": "738974b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 512)               857088    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 44)                22572     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 879,660\n",
      "Trainable params: 879,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load in the weights and show summary\n",
    "weights_GRU = GRU_model.get_weights()\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0021a302",
   "metadata": {
    "id": "cad5b34f"
   },
   "source": [
    "Finish the following function that uses a GRU architecture to generate text, given the weights of the GRU model, a text prompt, and the number of characters to return. The function should be completed by **only using numpy functions**. Use your knowledge of how every weight plays its role in the GRU architecture. Do not worry about the weight extraction part, this is already provided for you. The hidden state $h$ is initialized to a vector of zeros. \n",
    "\n",
    "The embedding of each character has to be done by a one-hot encoding, where you will need the dictionaries defined in the introduction to go from a character to an index position.\n",
    "\n",
    "Note: a slightly different version of the GRU is used, where the candidate state $c_t$ is calculated as:\n",
    "\n",
    "$$\n",
    "c_t = \\text{tanh} \\left(W_{hx} x_t \\ + \\ \\Gamma_t^r \\odot (W_{hh} h_{t-1}) \\ + \\ b_h \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cf2dca5",
   "metadata": {
    "id": "259c1d40"
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sample_text_GRU(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained GRU model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Total length of output sentence\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by GRU\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    output_sentence = prompt\n",
    "    \n",
    "    for i in range(N - len(prompt)):\n",
    "        x = np.zeros((W_ux.shape[1], 1))\n",
    "        current_char = output_sentence[-1] if output_sentence else ' '\n",
    "        x[char_to_indices[current_char]] = 1 \n",
    "        \n",
    "        \n",
    "        #update gate\n",
    "        z_t = sigmoid(np.dot(W_ux, x) + np.dot(W_uh, h) + b_u)\n",
    "        \n",
    "        #reset gate\n",
    "        r_t = sigmoid(np.dot(W_rx, x) + np.dot(W_rh, h) + b_r)\n",
    "        \n",
    "        #candidate state\n",
    "        c_t = np.tanh(np.dot(W_hx, x) + np.dot(W_hh, r_t * h) + b_h)\n",
    "        \n",
    "        # Update hidden state h_t\n",
    "        h = z_t * h + (1 - z_t) * c_t\n",
    "        \n",
    "        #output logits w/ softmax\n",
    "        y = np.dot(W_y, h) + b_y\n",
    "        exp_y = np.exp(y - np.max(y))\n",
    "        p = exp_y / np.sum(exp_y)\n",
    "        \n",
    "        # Sample the next character\n",
    "        next_char_index = np.random.choice(len(p), p=p.flatten())\n",
    "        next_char = indices_to_char[next_char_index]\n",
    "        \n",
    "        # Append the next character to the output sentence\n",
    "        output_sentence += next_char\n",
    "        \n",
    "        # Update the current character\n",
    "        current_char = next_char\n",
    "        \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609425e",
   "metadata": {
    "id": "117fdc8f"
   },
   "source": [
    "Test out your function by running the following code cell. Use it as a sanity check that your code is working. The generated text should not be perfect English, but at least you should be able to recognize some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01b7c6dd",
   "metadata": {
    "id": "3fc14584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrs. oliver looked at herself in the glass. she gave a brief, sideways looking evertto a dorlatingrea's of thosieny ot eliege to hadit. \"the a genellave in pole wait to to rote thin hemato enchad. ont a rowithin ather agke it bou?\" \"you well any herstyoul uss abler at, of a mothoug undee, aghin. \"newabted abroatht wid heriues, ats tok alorane sterielinglispelis notel, thot, ohing\" \"ahy ravenicvonit made part of a drewpentin -lied,\" same that, tory telli hed about,\" \"i's livits onewitatheri'te, he wad, \"or can susten, and han a it quite as and st ow woursly stouc you centay in tire.\" \"if toiris you know eve will alower the ore of he sents to awoult ress vertt. bethe  a teremolaboutery elonge thing. toryou all?\" \"yes that \" to any strolle a iepeatte and the was. alifilatin you?\" \"ahang the agria sotha fele the minute hings, and aboul, or onget ive, i timiritein ores, or in sestanly and a gidla is horsseee sidot elleded, bather and and abeatif thed howely? and all the peoplet notevevee te\n"
     ]
    }
   ],
   "source": [
    "print(sample_text_GRU(weights_GRU, \n",
    "                      'mrs. oliver looked at herself in the glass. she gave a brief, sideways look',\n",
    "                      1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a838312",
   "metadata": {
    "id": "6a35ba68"
   },
   "source": [
    "### Can Elephants Remember Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcbd3b2",
   "metadata": {
    "id": "09cd85c4"
   },
   "source": [
    "Perplexity is a measure to quantify how \"good\" a language model $M$ is, based on a test (or validation) set. The perplexity on a sequence $s$ of characters $a_i$ of size $N$ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity}(M) = M(s)^{(-1/N)} = \\left\\{p(a_1, \\ldots, a_N)\\right\\}^{(-1/N)} = \\left\\{p(a_1) \\ p(a_2|a_1) \\ \\ldots \\ p(a_N|a_1, \\ldots, a_{N-1})\\right\\}^{(-1/N)}\n",
    "$$\n",
    "\n",
    "> The intuition behind this metric is that, if a model assigns a high probability to a test set, it is not surprised to see it (not perplexed by it), which means the model $M$ has a good understanding of how the language works. Hence, a good model has, in theory, a lower perplexity. The exponent $(-1/N)$ in the formula is just a normalizing strategy (geometric average), because adding more characters to a test set would otherwise introduce more uncertainty (i.e. larger test sets would have lower probability). So by introducing the geometric average, we have a metric that is independent of the size of the test set.\n",
    "\n",
    "When calculating the perplexity, it is important to know that taking the product of a bunch of probabilities will most likely lead to a zero value by the computer. To prevent this, make use of a log-transformation:\n",
    "\n",
    "$$\n",
    "\\text{Log-Perplexity}(M) = -\\frac{1}{N} log\\left\\{p(a_1, \\ldots, a_N)\\right\\} = -\\frac{1}{N} \\left\\{log \\ p(a_1) + \\ log \\ p(a_2|a_1) + \\ \\ldots \\ + log \\  p(a_N|a_1, \\ldots, a_{N-1})\\right\\} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65e2fd",
   "metadata": {
    "id": "3ccf8083"
   },
   "source": [
    "There are noticeable differences between the text generated by the Vanilla RNN and the GRU models. When running both models multiple times with various prompts, the GRU consistently produces more coherent and contextually relevant continuations, maintaining the narrative flow and reducing repetitive phrases. In contrast, the Vanilla RNN often struggles with maintaining long-term dependencies, leading to repetitive or less meaningful text. This is also evident in the GRU output's punctuation and general sentence length. These differences are expected because GRUs incorporate gating mechanisms that effectively manage and retain important information over longer sequences, addressing the vanishing gradient problem inherent in Vanilla RNNs. As a result, GRUs demonstrate a superior ability to understand and generate complex language patterns, making them more robust for character-based language modeling tasks. It is clear that the GRU has lower perplexity than the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f260c36b",
   "metadata": {
    "id": "e0c2a390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Vanilla RNN: 1130.3767023652094\n",
      "Perplexity of GRU: 258.70324804219644\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "def sample_text_RNN(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained RNN to generate text, starting from a prompt, \n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained RNN model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Length of output sentence (including prompt)\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by RNN\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "\n",
    "    # First Recurrent Layer \n",
    "    W_xh1 = weights[0].T \n",
    "    W_h1h1 = weights[1].T \n",
    "    b_h1 = np.expand_dims(weights[2], axis=1)\n",
    "\n",
    "    # Second Recurrent Layer\n",
    "    W_h1h2 = weights[3].T\n",
    "    W_h2h2 = weights[4].T\n",
    "    b_h2 = np.expand_dims(weights[5], axis=1)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_h2y = weights[6].T\n",
    "    b_y = np.expand_dims(weights[7], axis=1)\n",
    "    \n",
    "    # Initiate the hidden states\n",
    "    h1 = np.zeros((W_h1h1.shape[0], 1))\n",
    "    h2 = np.zeros((W_h2h2.shape[0], 1))\n",
    "    \n",
    "    output_sentence = prompt\n",
    "    log_probs = 0\n",
    "    \n",
    "    for i in range(N - len(prompt)):\n",
    "        x = np.zeros((W_xh1.shape[1], 1))\n",
    "        current_char = output_sentence[-1] if output_sentence else ' '\n",
    "        x[char_to_indices[current_char]] = 1\n",
    "        \n",
    "        # Recurrent layers\n",
    "        h1 = np.tanh(np.dot(W_xh1, x) + np.dot(W_h1h1, h1) + b_h1)\n",
    "        h2 = np.tanh(np.dot(W_h1h2, h1) + np.dot(W_h2h2, h2) + b_h2)\n",
    "            \n",
    "        # Output\n",
    "        y = np.dot(W_h2y, h2) + b_y\n",
    "        exp_y = np.exp(y - np.max(y))\n",
    "        p = exp_y / np.sum(exp_y)\n",
    "        \n",
    "        if i < len(test_text) - len(prompt):\n",
    "            actual_next_char = test_text[len(prompt) + i]\n",
    "            actual_next_char_idx = char_to_indices[actual_next_char]\n",
    "            prob = p[actual_next_char_idx]\n",
    "            log_probs += np.log(prob + 1e-9) # avoid log(0)\n",
    "        \n",
    "        # Sample the next character based on the probability distribution\n",
    "        next_char_index = np.random.choice(len(p), p=p.flatten())\n",
    "        next_char = indices_to_char[next_char_index]\n",
    "        \n",
    "        output_sentence += next_char\n",
    "        current_char = next_char\n",
    "        \n",
    "    return output_sentence, log_probs\n",
    "\n",
    "\n",
    "def sample_text_GRU(weights, prompt, N):\n",
    "    '''\n",
    "    Uses a pretrained GRU to generate text, starting from a prompt,\n",
    "    only using the weights and numpy commands\n",
    "            Parameters:\n",
    "                    weights (list): Weights of the pretrained GRU model\n",
    "                    prompt (string): Start of generated sentence\n",
    "                    N (int): Total length of output sentence\n",
    "            Returns:\n",
    "                    output_sentence (string): Text generated by GRU\n",
    "    '''\n",
    "    # Extracting weights and biases\n",
    "    # Dimensions of matrices are same format as lecture slides\n",
    "    \n",
    "    # GRU Layer \n",
    "    W_ux, W_rx, W_hx = np.split(weights[0].T, 3, axis = 0)\n",
    "    W_uh, W_rh, W_hh = np.split(weights[1].T, 3, axis = 0)\n",
    "\n",
    "    bias = np.sum(weights[2], axis=0)\n",
    "    b_u, b_r, b_h = np.split(np.expand_dims(bias, axis=1), 3)\n",
    "\n",
    "    # Linear (dense) layer\n",
    "    W_y = weights[3].T\n",
    "    b_y = np.expand_dims(weights[4], axis=1)\n",
    "    \n",
    "    # Initiate hidden state\n",
    "    h = np.zeros((W_hh.shape[0], 1))\n",
    "    \n",
    "    output_sentence = prompt\n",
    "    log_probs = 0\n",
    "\n",
    "    \n",
    "    for i in range(N - len(prompt)):\n",
    "        x = np.zeros((W_ux.shape[1], 1))\n",
    "        current_char = output_sentence[-1] if output_sentence else ' '\n",
    "        x[char_to_indices[current_char]] = 1 \n",
    "        \n",
    "        \n",
    "        #update gate\n",
    "        z_t = sigmoid(np.dot(W_ux, x) + np.dot(W_uh, h) + b_u)\n",
    "        \n",
    "        #reset gate\n",
    "        r_t = sigmoid(np.dot(W_rx, x) + np.dot(W_rh, h) + b_r)\n",
    "        \n",
    "        #candidate state\n",
    "        c_t = np.tanh(np.dot(W_hx, x) + np.dot(W_hh, r_t * h) + b_h)\n",
    "        \n",
    "        # Update hidden state h_t\n",
    "        h = z_t * h + (1 - z_t) * c_t\n",
    "        \n",
    "        #output logits w/ softmax\n",
    "        y = np.dot(W_y, h) + b_y\n",
    "        exp_y = np.exp(y - np.max(y))\n",
    "        p = exp_y / np.sum(exp_y)\n",
    "        \n",
    "        if i < len(test_text) - len(prompt):\n",
    "            actual_next_char = test_text[len(prompt) + i]\n",
    "            actual_next_char_idx = char_to_indices.get(actual_next_char, None)\n",
    "            if actual_next_char_idx is not None:\n",
    "                prob = p[actual_next_char_idx]\n",
    "                log_probs += np.log(prob + 1e-9) # avoid log(0)\n",
    "        \n",
    "        # Sample the next character\n",
    "        next_char_index = np.random.choice(len(p), p=p.flatten())\n",
    "        next_char = indices_to_char[next_char_index]\n",
    "        \n",
    "        # Append the next character to the output sentence\n",
    "        output_sentence += next_char\n",
    "        \n",
    "        # Update the current character\n",
    "        current_char = next_char\n",
    "        \n",
    "    return output_sentence, log_probs\n",
    "\n",
    "def calculate_perplexity(model_function, weights, prompt, test_sequence):\n",
    "    generated_text, log_probs = model_function(weights, prompt, len(test_sequence))\n",
    "\n",
    "    perplexity = np.exp(-np.sum(log_probs) / len(test_sequence))\n",
    "    return perplexity\n",
    "                        \n",
    "\n",
    "prompt_length = 100\n",
    "prompt = test_text[:prompt_length]                       \n",
    "\n",
    "perplexity_RNN = calculate_perplexity(sample_text_RNN, weights_RNN, prompt, test_text)\n",
    "perplexity_GRU = calculate_perplexity(sample_text_GRU, weights_GRU, prompt, test_text)\n",
    "# Print results\n",
    "print(f\"Perplexity of Vanilla RNN: {perplexity_RNN}\")\n",
    "print(f\"Perplexity of GRU: {perplexity_GRU}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b7fe6",
   "metadata": {},
   "source": [
    "## Being the Bard\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a2/Shakespeare.jpg\" alt=\"William\" width=\"200\" style=\"float:left; padding:15px\"/>\n",
    "\n",
    "Transformer models are the current state of the art in many sequence modeling tasks, and the Transformer architecture underlies most Large Language Models (LLMs), including ChatGPT, Llama, Mistral, etc.\n",
    "\n",
    "In this problem, we will implement a Transformer language model from scratch in `numpy`. You will be provided with the weights of a small, slightly simplified Transformer language model that we trained on the works of Shakespeare. We will walk through implementing each component of the Transformer architecture and ultimately assemble this into a language model that can generate some text in the style of the Bard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e93cca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#!pip install tiktoken\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8019aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "n_layers = 4\n",
    "n_heads = 8\n",
    "d_ff = 1024\n",
    "vocab_size = 1024\n",
    "block_size = 128\n",
    "\n",
    "transformer_model_weights = np.load(f'problem2_model_parameters/model_weights_D{d_model}L{n_layers}H{n_heads}.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0401772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "    word_embedding.weight\n",
      "    position_embedding.inv_freq\n",
      "    layers.0.attention.wq.weight\n",
      "    layers.0.attention.wk.weight\n",
      "    layers.0.attention.wv.weight\n",
      "    layers.0.attention.wo.weight\n",
      "    layers.0.feed_forward.0.weight\n",
      "    layers.0.feed_forward.2.weight\n",
      "    layers.1.attention.wq.weight\n",
      "    layers.1.attention.wk.weight\n",
      "    layers.1.attention.wv.weight\n",
      "    layers.1.attention.wo.weight\n",
      "    layers.1.feed_forward.0.weight\n",
      "    layers.1.feed_forward.2.weight\n",
      "    layers.2.attention.wq.weight\n",
      "    layers.2.attention.wk.weight\n",
      "    layers.2.attention.wv.weight\n",
      "    layers.2.attention.wo.weight\n",
      "    layers.2.feed_forward.0.weight\n",
      "    layers.2.feed_forward.2.weight\n",
      "    layers.3.attention.wq.weight\n",
      "    layers.3.attention.wk.weight\n",
      "    layers.3.attention.wv.weight\n",
      "    layers.3.attention.wo.weight\n",
      "    layers.3.feed_forward.0.weight\n",
      "    layers.3.feed_forward.2.weight\n",
      "    fc_out.weight\n",
      "    fc_out.bias\n"
     ]
    }
   ],
   "source": [
    "print('Parameters:')\n",
    "for key in transformer_model_weights.keys():\n",
    "    print(f'    {key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c7b84",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "\n",
    "To process text with a neural network model, we need to first tokenize it in order to convert it to a numerical format that the model can understand and process. The tokenizer converts strings into sequences of integer tokens in a fixed vocabulary. There are different ways to do this. For this problem, we trained a custom [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)  (BPE) tokenizer on Shakespeare text, setting the vocabulary size to 1024. The code below demonstrates how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1705eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the BPE tokenizer\n",
    "with open('problem2_model_parameters/bpe1024_enc_full.pkl', 'rb') as pickle_file:\n",
    "    enc = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed255879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "What's in a name? that which we call a rose\n",
      "By any other name would smell as sweet;\n",
      "\n",
      "Encoded:\n",
      "[462, 320, 307, 258, 813, 63, 323, 621, 331, 800, 258, 697, 305, 10, 889, 801, 845, 813, 504, 260, 109, 408, 366, 818, 59]\n",
      "\n",
      "Tokens: \n",
      "['What', \"'s\", ' in', ' a', ' name', '?', ' that', ' which', ' we', ' call', ' a', ' ro', 'se', '\\n', 'By', ' any', ' other', ' name', ' would', ' s', 'm', 'ell', ' as', ' sweet', ';']\n",
      "\n",
      "Decoded text:\n",
      "What's in a name? that which we call a rose\n",
      "By any other name would smell as sweet;\n"
     ]
    }
   ],
   "source": [
    "# text to tokenize\n",
    "text = \"\"\"What's in a name? that which we call a rose\n",
    "By any other name would smell as sweet;\"\"\"\n",
    "\n",
    "print('Original text:')\n",
    "print(text)\n",
    "encoded = enc.encode(text)\n",
    "print()\n",
    "\n",
    "print('Encoded:')\n",
    "print(encoded)\n",
    "print()\n",
    "\n",
    "print('Tokens: ')\n",
    "print([enc.decode([idx]) for idx in encoded])\n",
    "decoded = enc.decode(encoded)\n",
    "print()\n",
    "\n",
    "print('Decoded text:')\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536a8cd",
   "metadata": {},
   "source": [
    "### Problem 2.2a: Token Embeddings\n",
    "\n",
    "After tokenization, we get a sequence of integers that represent the text to processed, with each integer index corresponding to a particular token in the vocabulary (e.g., a word or word-part). The first step in processing this text is to turn it into a vector representation. This is done via a learned embedding look-up table. For each token in the vocabulary $t \\in \\mathcal{V}$, we learn an embedding $E_t \\in {\\mathbb R}^d$. A sequence of tokens $(t_1, ..., t_n)$ is transformed to a vector representation by mapping each token to its embedding $(E_{t_1}, ..., E_{t_n}) \\in {\\mathbb R}^{n \\times d}$. This sequence of vectors is what the neural network model ultimately operates over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d3c9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_tokens(tokens, params):\n",
    "    \"\"\"\n",
    "    Embed tokens using the input embeddings.\n",
    "\n",
    "    Args:\n",
    "        tokens (np.array): array of token indices, shape (n_tokens,)\n",
    "        params (dict): dictionary containing the model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # get needed parameters\n",
    "    embeddings = params['word_embedding.weight'] # shape (vocab_size, d_model)\n",
    "    # embeddings is a look up table for embeddings, with rows corresponding to token indices\n",
    "    # i.e., embeddings[token_index] returns the embedding for the token with index token_index\n",
    "\n",
    "    # look up embeddings\n",
    "    embedded_tokens = embeddings[tokens]\n",
    "\n",
    "    return embedded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6dc75af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4494016 ,  0.8637606 ,  0.64816946, -1.0005027 ,  0.65828127],\n",
       "       [ 1.1595719 , -0.09202019,  1.4256238 ,  1.7668523 , -1.366581  ],\n",
       "       [ 0.7071919 ,  0.45128715, -1.1132193 , -0.18074755, -0.36634383]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tokens(np.array([1, 2, 3]), params=transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b3e99",
   "metadata": {},
   "source": [
    "Expected answer:\n",
    "\n",
    "```\n",
    "array([[-0.4494016 ,  0.8637606 ,  0.64816946, -1.0005027 ,  0.65828127],\n",
    "       [ 1.1595719 , -0.09202019,  1.4256238 ,  1.7668523 , -1.366581  ],\n",
    "       [ 0.7071919 ,  0.45128715, -1.1132193 , -0.18074755, -0.36634383]],\n",
    "      dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff4c27",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Transformer models are by-default permutation-equivariant. That is, they don't understand order or position. To make them understand positional information, we need to encode it directly in the token embeddings. One way to do this is to represent each possible position $i$ with its own position embedding ${PE}_i \\in \\mathbb{R}^{d}$. One way to encode the position of each token is to simply add a positional embedding representing the position of the token.\n",
    "\n",
    "In the original Transformer paper, the authors propose a particular choice for ${PE}_i \\in {\\mathbb R}^{d}$ based on sines and cosines with frequencies depending on the position $i$. Since the original proposal, many follow-up works proposed different positional encoding methods aiming to improve performance and length-generalization. In this problem, we'll use the sinusoidal positional encodings of the original Transformer paper.\n",
    "\n",
    "We provide the code for computing these sinusoidal positional embeddings below. To give some intuition about the structure of the sinusoidal positional embeddings, we also plot a heatmap of the pairwise inner products $\\langle PE_i, PE_j \\rangle$. We see that that positions that are closer together have more similar positional embeddings, with additional oscillatory behavior on top of that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215faef",
   "metadata": {},
   "source": [
    "Below is an implementation of the positional embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0f9aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoidal_positional_embeddings(sequence_length, dim, base=10000):\n",
    "    inv_freq = 1.0 / (base ** (np.arange(0, dim, 2) / dim))\n",
    "    t = np.arange(sequence_length)\n",
    "    sinusoid_inp = np.einsum(\"i,j->ij\", t, inv_freq)\n",
    "\n",
    "    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)\n",
    "\n",
    "    emb = np.concatenate((sin, cos), axis=-1)\n",
    "    # return emb[None, :, :]\n",
    "    return emb[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "70f1014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.84147098  0.8019618   0.76172041  0.72141412  0.68156135  0.64255691\n",
      "   0.60469402  0.56818322  0.53316844  0.49974046]\n",
      " [ 0.90929743  0.95814438  0.98704625  0.9991642   0.99748     0.984703\n",
      "   0.96322662  0.9351183   0.90213071  0.86572559]\n",
      " [ 0.14112001  0.34278182  0.51730572  0.66243612  0.77827252  0.86647672\n",
      "   0.92964484  0.97083837  0.99325317  0.9999996 ]\n",
      " [-0.7568025  -0.54860557 -0.31671543 -0.08168498  0.14153892  0.34315172\n",
      "   0.51761928  0.66269154  0.77847174  0.8666241 ]]\n"
     ]
    }
   ],
   "source": [
    "print(pe[:5, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4975426",
   "metadata": {},
   "source": [
    "Explore the positional embeddings by computing the inner-product between all pairs of positional embeddings, and then plotting the similarity matrix as a heat map. Do the results make sense? What are the positional embeddings designed to model? Do they do this effectively? Comment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3ddc46dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pe = get_sinusoidal_positional_embeddings(128, 256)\n",
    "\n",
    "pe_similarity = np.dot(pe, pe.T)\n",
    "\n",
    "sns.heatmap(pe_similarity)\n",
    "plt.title('Similarity Structure of Sinusoidal Positional Embeddings')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92850d0c",
   "metadata": {},
   "source": [
    "The heatmap of the pairwise inner products of the sinusoidal positional embeddings reveals a clear structure where positions that are closer together exhibit higher similarity, as evidenced by brighter regions along the diagonal. This pattern is a direct consequence of the sinusoidal functions used to generate the embeddings, which ensure that nearby positions have embeddings with similar phase and frequency components. Additionally, the oscillatory behavior in the similarity matrix reflects the multi-frequency nature of the sinusoidal encodings, allowing the model to capture both local and global positional relationships. These embeddings are specifically designed to encode positional information in a way that preserves the order and relative distances between tokens, enabling the Transformer model to effectively utilize sequence order without being inherently aware of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc783c84",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "The core operation in a Transformer is multi-head attention, sometimes called self-attention. In this problem, we will implement multi-head attention in numpy from scratch, given the trained parameters of the model.\n",
    "\n",
    "The input is a sequence of vectors $X = (x_1, ..., x_n) \\in {\\mathbb R}^{n \\times d_{model}}$. For each attention head $h \\in [n_h]$, the parameters consist of a query projection matrix $W_q^h \\in {\\mathbb R}^{d_{model} \\times d_h}$, a key projection matrix $W_k^h \\in {\\mathbb R}^{d_{model} \\times d_h}$, and a value projection matrix $W_k^h \\in {\\mathbb R}^{d_{model} \\times d_h}$. Here, $d_h$ is the \"head dimension\", taken to be $d_{model} / n_h$ (to maintain the same dimensionality between the input and output). The algorithm, for each head, is the following:\n",
    "1. Compute the queries $Q = X W_q^h$, keys $K = X W_k^h$, and values $V = X W_v^h$. Note that the linear maps are applied independently for each token across the embedding dimension (not sequence dimension), such that $Q, K, V \\in {\\mathbb R}^{n \\times d_h}$.\n",
    "2. Compare the queries and keys via inner products to get an $n \\times n$ attention matrix $A = \\mathrm{Softmax}(Q K^{\\intercal} / \\sqrt{d_h}) \\in {\\mathbb R}^{n \\times n}$.\n",
    "3. Use the attention scores $A$ to select values, producing the output of the self-attention head: $\\mathrm{head}_h = A V \\in {\\mathbb R}^{n \\times d_h}$.\n",
    "We then concatenate the retrieved values across all heads, and apply a final linear map. Putting this all together yields:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathrm{head}_h &= \\mathrm{Softmax}((X W_q^h) (X W_k^h)^{\\intercal}/ \\sqrt{d_h}) X W_v^h\\\\\n",
    "    \\mathrm{MultiHeadAttention}(X) &= \\mathrm{concat}(\\mathrm{head}_1, ..., \\mathrm{head}_{n_h}) W_o\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Note that the matrices $W_q^h$, $W_k^h$, $W_v^h$ are \"packed together\" across heads when you read in the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a33c8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we provide a couple of utility functions\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    # a stable implementation of the softmax function\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def apply_presoftmax_causal_mask(attn_scores):\n",
    "    # apply a causal mask to the attention scores (set the entries above the diagonal to -inf)\n",
    "    n = attn_scores.shape[-1]\n",
    "    mask = np.triu(np.ones((n, n)), k=1)\n",
    "    masked_scores = attn_scores - 1e9 * mask\n",
    "    return masked_scores\n",
    "\n",
    "def multi_head_attention(x, params, layer_prefix='layers.0'):\n",
    "    \"\"\"\n",
    "    Compute multi-head self-attention.\n",
    "\n",
    "    Args:\n",
    "        x (np.array): input tensor, shape (n, d_model)\n",
    "        params (dict): dictionary containing the model parameters\n",
    "        layer_prefix (str): prefix of parameter names corresponding to the layer\n",
    "        verbose (bool): whether to print intermediate shapes\n",
    "    \"\"\"\n",
    "\n",
    "    # get parameters of multi-head attention layer\n",
    "    wq = params[f'{layer_prefix}.attention.wq.weight'].T # (d_model, d_model)\n",
    "    wk = params[f'{layer_prefix}.attention.wk.weight'].T # (d_model, d_model)\n",
    "    wv = params[f'{layer_prefix}.attention.wv.weight'].T # (d_model, d_model)\n",
    "    wo = params[f'{layer_prefix}.attention.wo.weight'].T # (d_model, d_model)\n",
    "\n",
    "    head_dim = d_model // n_heads # dimension of each head\n",
    "    attn_scale = 1 / math.sqrt(head_dim) # scaling factor for attention scores\n",
    "\n",
    "    # the wq, wk, wv, wo matrices contain weights for all heads, concatenated\n",
    "    # first, we split wq, wk, wv, wo into heads\n",
    "    # note: there are more efficient implementations, but this is more verbose/pedagogical\n",
    "    wq = wq.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "    wk = wk.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "    wv = wv.reshape(d_model, n_heads, head_dim).transpose(1, 0, 2) # (n_heads, d_model, head_dim)\n",
    "\n",
    "    head_outputs = []\n",
    "    for head in range(n_heads):\n",
    "\n",
    "        # get head-specific parameters (these are the query/key/value projections for this head)\n",
    "        wqh = wq[head] # (d_model, head_dim)\n",
    "        wkh = wk[head] # (d_model, head_dim)\n",
    "        wvh = wv[head] # (d_model, head_dim)\n",
    "\n",
    "        # compute queries, keys, values\n",
    "        # your code here\n",
    "        q = x.dot(wqh) # (n, head_dim)\n",
    "        k = x.dot(wkh) # (n, head_dim)\n",
    "        v = x.dot(wvh) # (n, head_dim)\n",
    "\n",
    "        # compute attention scores\n",
    "        # your code here\n",
    "        attn_scores = q.dot(k.T) * attn_scale # (n, n)\n",
    "\n",
    "        attn_scores = apply_presoftmax_causal_mask(attn_scores)\n",
    "        attn_weights = softmax(attn_scores, axis=-1) # (n, n)\n",
    "\n",
    "        # apply attention scores to values\n",
    "        # your code here\n",
    "        head_out = attn_weights.dot(v) # (n, head_dim)\n",
    "\n",
    "        # store the head output\n",
    "        head_outputs.append(head_out)\n",
    "\n",
    "    # concatenate all head outputs\n",
    "    head_outputs = np.concatenate(head_outputs, axis=-1) # (n, d_model)\n",
    "\n",
    "    # apply output linear map W_o to concatenated head outputs\n",
    "    output = head_outputs.dot(wo)  # (n, d_model)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f08934d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
       "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
       "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attention(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7e60b",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "array([[ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
    "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ],\n",
    "       [ 0.48334133,  0.19740422, -0.39514927, -0.40647455,  0.4831646 ]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5262bb79",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Each Transformer layer (i.e., block) consists of two operations: 1) (multi-head) self-attention, which enables exchange of information between tokens, and 2) a multi-layer perceptron, which processes each token independently. A Transformer model is essentially just alternating between these two operations. In this problem, we will implement the multi-layer perceptron step. Typically, the MLP at each layer is simply a two-layer (one hidden layer) MLP or Feed Forward Network. In our model, we use a ReLU activation in the hidden layer, though other activations are possible. The same MLP network is applied to each token embedding in the sequence independently. \n",
    "\n",
    "Given $X = (x_1, ..., x_n) \\in {\\mathbb R}^{n \\times d_{model}}$, we apply the MLP as follows:\n",
    "\n",
    "$$\\mathrm{MLP}(X) = \\mathrm{ReLU}(X W_1) W_2$$\n",
    "\n",
    "Note that we don't use biases for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1bab44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def mlp(x, params, layer_prefix='layers.0'):\n",
    "    # get MLP parameters\n",
    "    w1 = params[f'{layer_prefix}.feed_forward.0.weight'].T # (d_model, d_ff)\n",
    "    w2 = params[f'{layer_prefix}.feed_forward.2.weight'].T # (d_ff, d_model)\n",
    "\n",
    "    o = relu(x.dot(w1)).dot(w2) # (n, d_model)\n",
    "\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c029d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
       "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
       "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77c039",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "array([[0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
    "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956],\n",
    "       [0.06068574, 0.6727857 , 0.20872724, 0.42208509, 0.29517956]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e10058",
   "metadata": {},
   "source": [
    "### Final Prediction Layer\n",
    "\n",
    "A Transformer model iteratively applies multi-head attention and MLP layers to process the input. This produces a processed representation of shape $n \\times d_{model}$. To make the final prediction (e.g., predict the next token), we need to map the $d_{model}$-dimensional embedding vectors to logits over the output vocabulary. To do this, we simply apply a linear map that maps from $d_{model}$ to $\\mathtt{vocab\\_size}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "682c8f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_head(x, params):\n",
    "    # get needed parameters\n",
    "    w = params['fc_out.weight'].T # (d_model, vocab_size)\n",
    "    b = params['fc_out.bias'] # (vocab_size,)\n",
    "\n",
    "    logits = x.dot(w) + b # (n, vocab_size)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dc2486eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
       "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
       "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_head(np.ones((block_size, d_model)), transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703eaab",
   "metadata": {},
   "source": [
    "```\n",
    "array([[ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
    "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351],\n",
    "       [ 0.88448969,  0.07200013, -0.67318526, -1.11558351,  0.14410351]])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13656f53",
   "metadata": {},
   "source": [
    "### Putting it all together: A Full Transformer Language Model\n",
    "\n",
    "We are now ready to put this all together to assemble our Transformer Language Model. Recall that the Transformer architecture consists of iteratively applying multi-head attention and MLPs. Each time we apply attention or the MLP, we also apply a *residual connection*: $X^{(\\ell + 1)} = X^{(\\ell)} + F(X^{(\\ell)})$. This can be interpreted as a mechanism to enable easy communication between different layers (some people call refer to this idea as the \"residual stream\"). Real Transformers also include layer normalization in each layer, but we omit this for simplicity in this problem.\n",
    "\n",
    "The full algorithm is given below:\n",
    "1. Embed the tokens using the embedding lookup table: $(t_1, ..., t_n) \\mapsto (E_{t_1}, ..., E_{t_n}) =: X^{(0)}$\n",
    "2. Add the positional embeddings: $X^{(0)} \\gets X^{(0)} + (PE_1, ..., PE_n)$\n",
    "3. For each layer $\\ell = 1, ..., L$:\n",
    "    1. Apply Multi-Head Attention: $\\tilde{X}^{(\\ell)} \\gets X^{(\\ell-1)} + \\mathrm{MultiHeadAttention}(X^{(\\ell-1)})$.\n",
    "    2. Apply the MLP: $X^{(\\ell)} \\gets \\tilde{X}^{(\\ell)} + \\mathrm{MLP}(\\tilde{X}^{(\\ell)})$.\n",
    "4. Compute the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79983203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(tokens, params):\n",
    "    # tokens: (n,) integer array\n",
    "    # params: dictionary of parameters\n",
    "\n",
    "    # map tokens to embeddings using embed_tokens\n",
    "    x = embed_tokens(tokens, params)  # (n, d_model)\n",
    "\n",
    "    # add positional embeddings\n",
    "    pe = get_sinusoidal_positional_embeddings(x.shape[0], x.shape[1]) # (n, d_model)\n",
    "    # your code here\n",
    "    x = x + pe\n",
    "\n",
    "    # transformer blocks\n",
    "    for i in range(n_layers):\n",
    "\n",
    "        # compute multi-head self-attention and add residual\n",
    "        # your code here\n",
    "        attn_out = multi_head_attention(x, params, layer_prefix=f'layers.{i}') # (n, d_model)\n",
    "        # your code here\n",
    "        x = x + attn_out\n",
    "\n",
    "        # compute MLP and add residual\n",
    "        mlp_out = mlp(x, params, layer_prefix=f'layers.{i}') # (n, d_model)\n",
    "        # your code here\n",
    "        x = x + mlp_out\n",
    "\n",
    "    # compute logits via the prediction_head\n",
    "    logits = prediction_head(x, params) # (n, vocab_size)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f2a13d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.9641349 , -5.12566872, -5.90677718, -5.57889839, -3.85043564],\n",
       "       [-2.31297379, -4.9703405 , -3.49086668, -5.3996587 , -3.99684942],\n",
       "       [-2.97001726, -4.84049568, -4.04949194, -4.01892107, -6.03805991]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer([0, 1, 2], params=transformer_model_weights)[:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3af6d",
   "metadata": {},
   "source": [
    "Expected Output:\n",
    "\n",
    "```\n",
    "array([[-1.96413494, -5.12566872, -5.90677725, -5.57889829, -3.85043572],\n",
    "       [-2.31297382, -4.97034061, -3.49086669, -5.39965866, -3.99684957],\n",
    "       [-2.97001738, -4.8404957 , -4.04949199, -4.01892112, -6.03806011]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8140e28",
   "metadata": {},
   "source": [
    "### Generate some text\n",
    "\n",
    "Below, we provide some code for generating text from a Transformer language model. The sampling procedure is *autoregressive*. This means that we input some text to the model and it outputs a distribution over next tokens. We sample the next token and append it to the text, then repeat the procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b513e",
   "metadata": {},
   "source": [
    "Complete the next token generator, but filling in the missing code below. This uses \"temperature\" to focus on the more probable tokens in a given context (as the temperature decreases). This results in sampling according to \n",
    "$ \\mathrm{Softmax}(\\mathrm{logits}/T)$ where $T \\geq 0$ is the temperature; lower temperature places higher probability on tokens having larger logits. Greedy sampling corresponds to $T=0$, and selects the token with the largest logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "62cc0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_transformer(prefix_text, params, max_len=128, greedy=False, temperature=0.9):\n",
    "    # encode seed text\n",
    "    prefix_tokens = list(enc.encode(prefix_text))\n",
    "\n",
    "    # initialize generated tokens\n",
    "    generated_tokens = prefix_tokens\n",
    "\n",
    "    # generate new tokens\n",
    "    for i in range(max_len):\n",
    "        # predict next token\n",
    "        logits = transformer(generated_tokens, params)\n",
    "        # logits[-1] corresponds to prediction of the next token\n",
    "        if greedy:\n",
    "            next_token = np.argmax(logits[-1])\n",
    "        else:\n",
    "            next_token = np.random.choice(len(softmax(logits[-1] / temperature)), p=softmax(logits[-1] / temperature))\n",
    "\n",
    "        # add next token to generated tokens\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "    # This converts the tokens to text, using the tiktoken decoder:\n",
    "    generated_text = enc.decode(generated_tokens)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba145d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTONIO:\n",
      "Do you not hear me speak?\n",
      "\n",
      "First Citizen:\n",
      "We cannot\n",
      "\n",
      "First Citizen:\n",
      "He that hath done fell, sir, sir, sir, sir, sir, sir, sir, sir,\n",
      "Which ne'er could the belly answer'd--\n",
      "Which you,\n",
      "Which you shall fell'd:\n",
      "Which you shall tell you'll hear it smile, Iliber,\n",
      "Which ne'--it it belly taunted head the belly,\n",
      "MENENIUS:\n",
      "There answer'd, you'll have lusedup\n",
      "'\n",
      "There\n"
     ]
    }
   ],
   "source": [
    "prefix_text = \"\"\"ANTONIO:\n",
    "Do you not hear me speak?\"\"\"\n",
    "\n",
    "generated_text = generate_with_transformer(prefix_text, transformer_model_weights, greedy=True, max_len=128)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fdbfc520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTONIO:\n",
      "Do you not hear me speak?\n",
      "\n",
      "First Citizen:\n",
      "If you muthorant eye!\n",
      "\n",
      "They, we'll hear the belly answer.\n",
      "\n",
      "\n",
      "Ally tale:\n",
      "Sir countrymen, being one that art wored!\n",
      "'er came from the lungs, but even thus--\n",
      "When you\n",
      "For, I may make the belly smile\n",
      "As well as speak--it t'--it tauntingly replied\n",
      "To the discontented\n",
      "cons, the dis\n"
     ]
    }
   ],
   "source": [
    "prefix_text = \"\"\"ANTONIO:\n",
    "Do you not hear me speak?\"\"\"\n",
    "\n",
    "generated_text = generate_with_transformer(prefix_text, transformer_model_weights,\n",
    "    greedy=False, temperature=0.8, max_len=128)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff574506",
   "metadata": {},
   "source": [
    "In the first case, we are using a greedy approach, so the content lacks coherence and often falls into repetitive patterns, such as the repeated use of \"sir,\" which disrupts the flow and authenticity of the dialogue. When using temperature sampling with a temperature of 0.8, there is slightly more variation and creativity, but the sentences remain fragmented and occasionally nonsensical. This indicates that while the model grasps some stylistic aspects of Shakespeare's works, it struggles with maintaining logical consistency and context. To enhance the quality of the generated text, increasing the model's complexity by adding more layers or attention heads could provide a deeper understanding of language patterns. Additionally, incorporating techniques like layer normalization, better regularization, and training on a larger and more diverse dataset would likely improve coherence and reduce repetitive outputs, leading to more fluent and contextually appropriate generations."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
